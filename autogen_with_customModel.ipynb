{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724a53e-b728-4750-8351-ef93265d6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,GenerationConfig\n",
    "\n",
    "from together import Together\n",
    "import os\n",
    "# Set the environment variable (or ensure it's already set in the system)\n",
    "os.environ['TOGETHER_API_KEY'] = ''\n",
    "\n",
    "client = Together()\n",
    "MODEL = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
    "# login to Hugging Face Hub\n",
    "\n",
    "access_token_write=''\n",
    "login(token=access_token_write)\n",
    "\n",
    "# custom client with custom model loader\n",
    "os.environ[\"OAI_CONFIG_LIST\"] = json.dumps(\n",
    "    [\n",
    "        {\n",
    "            \"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            \"model_client_cls\": \"CustomModelClient\",\n",
    "            \"device\": 0,\n",
    "            \"n\": 1,\n",
    "            \"params\": {\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"top_k\": 50,\n",
    "                \"temperature\": 0.8,\n",
    "                \"do_sample\": True,\n",
    "                \"return_full_text\": False,\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# custom user client with custom model loader\n",
    "os.environ[\"OAI_CONFIG_LISTs\"] = json.dumps(\n",
    "    [\n",
    "        {\n",
    "            \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            \"model_client_cls\": \"CustomModelClient2\",\n",
    "            \"device\": 0,\n",
    "            \"n\": 1,\n",
    "            \"params\": {\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"top_k\": 50,\n",
    "                \"temperature\": 0.8,\n",
    "                \"do_sample\": True,\n",
    "                \"return_full_text\": False,\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "class CustomModelClient:\n",
    "    def __init__(self, config, **kwargs):\n",
    "        print(f\"CustomModelClient config: {config}\")\n",
    "        self.device = config.get(\"device\", \"cuda\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(config[\"model\"]).to(self.device)\n",
    "        self.model_name = config[\"model\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config[\"model\"], use_fast=False)\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        # params are set by the user and consumed by the user since they are providing a custom model\n",
    "        # so anything can be done here\n",
    "        gen_config_params = config.get(\"params\", {})\n",
    "        self.max_length = gen_config_params.get(\"max_length\", 256)\n",
    "\n",
    "        print(f\"Loaded model {config['model']} to {self.device}\")\n",
    "\n",
    "    def create(self, params):\n",
    "        if params.get(\"stream\", False) and \"messages\" in params:\n",
    "            raise NotImplementedError(\"Local models do not support streaming.\")\n",
    "        else:\n",
    "            num_of_responses = params.get(\"n\", 1)\n",
    "\n",
    "            # can create my own data response class\n",
    "            # here using SimpleNamespace for simplicity\n",
    "            # as long as it adheres to the ClientResponseProtocol\n",
    "\n",
    "            response = SimpleNamespace()\n",
    "\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                params[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True\n",
    "            ).to(self.device)\n",
    "            inputs_length = inputs.shape[-1]\n",
    "\n",
    "            # add inputs_length to max_length\n",
    "            max_length = self.max_length + inputs_length\n",
    "            generation_config = GenerationConfig(\n",
    "                max_length=max_length,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            response.choices = []\n",
    "            response.model = self.model_name\n",
    "\n",
    "            for _ in range(num_of_responses):\n",
    "                outputs = self.model.generate(inputs, generation_config=generation_config)\n",
    "                # Decode only the newly generated text, excluding the prompt\n",
    "                text = self.tokenizer.decode(outputs[0, inputs_length:])\n",
    "                choice = SimpleNamespace()\n",
    "                choice.message = SimpleNamespace()\n",
    "                choice.message.content = text\n",
    "                choice.message.function_call = None\n",
    "                response.choices.append(choice)\n",
    "\n",
    "            return response\n",
    "\n",
    "    def message_retrieval(self, response):\n",
    "        \"\"\"Retrieve the messages from the response.\"\"\"\n",
    "        choices = response.choices\n",
    "#         response = client.chat.completions.create(\n",
    "#         model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "#         messages=[{\"role\": \"user\", \"content\": f\"concise the response: {choices}\"}],\n",
    "#         temperature=0.8,\n",
    "#         top_p=0.9,\n",
    "# )\n",
    "        return [choice.message.content for choice in choices]\n",
    "\n",
    "    def cost(self, response) -> float:\n",
    "        \"\"\"Calculate the cost of the response.\"\"\"\n",
    "        response.cost = 0\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_usage(response):\n",
    "        # returns a dict of prompt_tokens, completion_tokens, total_tokens, cost, model\n",
    "        # if usage needs to be tracked, else None\n",
    "        return {}\n",
    "\n",
    "class CustomModelClient2:\n",
    "    def __init__(self, config, **kwargs):\n",
    "        print(f\"CustomModelClient config: {config}\")\n",
    "        self.device = config.get(\"device\", \"cuda\")\n",
    "        self.history = []  # Store the history of all messages\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(config[\"model\"]).to(self.device)\n",
    "        self.model_name = config[\"model\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config[\"model\"], use_fast=False)\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        # params are set by the user and consumed by the user since they are providing a custom model\n",
    "        # so anything can be done here\n",
    "        gen_config_params = config.get(\"params\", {})\n",
    "        self.max_length = gen_config_params.get(\"max_length\", 256)\n",
    "\n",
    "        print(f\"Loaded model {config['model']} to {self.device}\")\n",
    "\n",
    "    def create(self, params):\n",
    "        if params.get(\"stream\", False) and \"messages\" in params:\n",
    "            raise NotImplementedError(\"Local models do not support streaming.\")\n",
    "        else:\n",
    "            num_of_responses = params.get(\"n\", 1)\n",
    "\n",
    "            response = SimpleNamespace()\n",
    "\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                params[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True\n",
    "            ).to(self.device)\n",
    "            inputs_length = inputs.shape[-1]\n",
    "\n",
    "            # add inputs_length to max_length\n",
    "            max_length = self.max_length + inputs_length\n",
    "            generation_config = GenerationConfig(\n",
    "                max_length=max_length,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            response.choices = []\n",
    "            response.model = self.model_name\n",
    "\n",
    "            for _ in range(num_of_responses):\n",
    "                outputs = self.model.generate(inputs, generation_config=generation_config)\n",
    "                # Decode only the newly generated text, excluding the prompt\n",
    "                text = self.tokenizer.decode(outputs[0, inputs_length:])\n",
    "                choice = SimpleNamespace()\n",
    "                choice.message = SimpleNamespace()\n",
    "                choice.message.content = text\n",
    "                choice.message.function_call = None\n",
    "                response.choices.append(choice)\n",
    "\n",
    "            return response\n",
    "\n",
    "    def add_to_history(self, role, message):\n",
    "        \"\"\"Add a message to the history.\"\"\"\n",
    "        self.history.append({\"role\": role, \"content\": message})\n",
    "\n",
    "    def send_message(self, message):\n",
    "        \"\"\"Send a message and add it to history.\"\"\"\n",
    "        self.add_to_history(\"user_proxy\", message)\n",
    "\n",
    "    def get_user_history(self):\n",
    "        \"\"\"Retrieve only user messages from the history.\"\"\"\n",
    "        return [entry[\"content\"] for entry in self.history if entry[\"role\"] == \"user_proxy\"]\n",
    "\n",
    "    def message_retrieval(self, response):\n",
    "        \"\"\"Retrieve the messages from the response.\"\"\"\n",
    "        choices = response.choices\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"concise the response: {choices}\"}],\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        return [response.choices[0].message.content]\n",
    "\n",
    "    def cost(self, response) -> float:\n",
    "        \"\"\"Calculate the cost of the response.\"\"\"\n",
    "        response.cost = 0\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_usage(response):\n",
    "        # returns a dict of prompt_tokens, completion_tokens, total_tokens, cost, model\n",
    "        # if usage needs to be tracked, else None\n",
    "        return {}\n",
    "\n",
    "# ########################################################################################\n",
    "system_message = \"\"\"You are a world-class AI-based coach.\"\"\"\n",
    "user_message = \"\"\"You are a USER with some challenges, and you are here to seek help from a world-class AI-based coach.\"\"\"\n",
    "\n",
    "config_list_custom = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\"model_client_cls\": [\"CustomModelClient\"]},\n",
    ")\n",
    "config_list_custom2 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LISTs\",\n",
    "    filter_dict={\"model_client_cls\": [\"CustomModelClient2\"]},\n",
    ")\n",
    "assistant = AssistantAgent(\"assistant\",  system_message=system_message,llm_config={\"config_list\": config_list_custom},human_input_mode=\"NEVER\")\n",
    "user_proxy = UserProxyAgent(\"user_proxy\",system_message=user_message, llm_config={\"config_list\": config_list_custom2}, max_consecutive_auto_reply=5,code_execution_config=False,human_input_mode=\"NEVER\")\n",
    "assistant.register_model_client(model_client_cls=CustomModelClient)\n",
    "user_proxy.register_model_client(model_client_cls=CustomModelClient2)\n",
    "\n",
    "assistant.initiate_chat(user_proxy, message=\"Hello, welcome to MetaMindful! How are you feeling today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951e2ab2-4f05-40e1-b972-4562b5a63ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'requirements.txt' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Specify the output file name\n",
    "requirements_file = \"requirements.txt\"\n",
    "\n",
    "# Generate requirements.txt using pip freeze\n",
    "try:\n",
    "    with open(requirements_file, \"w\") as file:\n",
    "        # Use subprocess to call pip freeze and write the output to the file\n",
    "        result = subprocess.run([\"pip\", \"freeze\"], stdout=file, check=True)\n",
    "    print(f\"'{requirements_file}' has been created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68daa17-e905-4c4b-8024-ab19198500e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
